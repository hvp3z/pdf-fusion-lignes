"""
Extracteur Comptable IA
Application Streamlit pour extraire des lignes comptables depuis des relev√©s bancaires PDF
via Gemini 1.5 Flash et les exporter en Excel.
"""

import streamlit as st
import fitz  # PyMuPDF
import google.generativeai as genai
import pandas as pd
import json
import io
import re
import time
from dotenv import load_dotenv
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

# Charger les variables d'environnement
load_dotenv()

# Configuration de la page Streamlit
st.set_page_config(
    page_title="Extracteur Comptable IA",
    page_icon="üìä",
    layout="wide"
)

# Constantes
MAX_FILES = 15
GEMINI_MODEL = "gemini-2.5-flash"  # Mise √† jour : gemini-1.5-flash n'est plus disponible

# Prompt syst√®me pour Gemini
SYSTEM_PROMPT = """Tu es un assistant comptable expert sp√©cialis√© dans l'analyse de relev√©s bancaires fran√ßais.

MISSION : Analyse le relev√© bancaire fourni et extrais TOUTES les lignes de transactions, SANS EN OMETTRE AUCUNE.

Pour chaque transaction, retourne un objet JSON avec :
- "date": la date de la transaction au format JJ/MM/AAAA (IMPORTANT: si la date dans le PDF est au format JJ/MM seulement, reconstitue la date compl√®te en utilisant l'ann√©e mentionn√©e dans l'en-t√™te du relev√©, par exemple "Arr√™t√© mensuel du 1 au 30 avril 2025" indique que l'ann√©e est 2025)
- "libelle": le libell√© de l'op√©ration. Si le libell√© d√©passe 50 caract√®res, r√©sume-le de mani√®re concise en gardant les mots-cl√©s essentiels (nom du b√©n√©ficiaire, type d'op√©ration, r√©f√©rence importante).
- "debit": le montant en d√©bit sous forme de nombre flottant (ex: 1234.56). Mettre null si c'est un cr√©dit.
- "credit": le montant en cr√©dit sous forme de nombre flottant (ex: 1234.56). Mettre null si c'est un d√©bit.

R√àGLES IMPORTANTES :
1. NORMALISATION DES MONTANTS : Convertis tous les formats de montants en nombres flottants standard.
   - "1 000,50" ‚Üí 1000.50
   - "1.000,50" ‚Üí 1000.50
   - "1,000.50" ‚Üí 1000.50
   - "1000,50" ‚Üí 1000.50
2. DISTINCTION D√âBIT/CR√âDIT (CRITIQUE) :
   - D√âBIT (sortie d'argent) : paiements CB, pr√©l√®vements, virements VERS quelqu'un
   - CR√âDIT (entr√©e d'argent) : virements RE√áUS, remboursements
   - ATTENTION aux virements : "VIREMENT A [nom]" ou "VIREMENT INSTANTANE A [nom]" = D√âBIT (argent qui SORT)
   - "VIREMENT DE [nom]" ou "VIREMENT INSTANTANE DE [nom]" = CR√âDIT (argent qui ENTRE)
   - Dans le PDF, le montant est g√©n√©ralement dans la colonne D√©bit ou Cr√©dit - respecte cette position.
3. Ignore les lignes qui ne sont pas des transactions (soldes, totaux, en-t√™tes, etc.).
4. Si une transaction s'√©tend sur plusieurs lignes dans le PDF, reconstitue-la correctement.
5. CRITIQUE : Extrais ABSOLUMENT TOUTES les transactions, y compris celles en fin de relev√©. Ne tronque pas ta r√©ponse m√™me si elle est longue.
6. Les dates peuvent √™tre au format JJ/MM dans le PDF - reconstitue-les en JJ/MM/AAAA en utilisant l'ann√©e du relev√©.
7. R√©ponds UNIQUEMENT avec un tableau JSON valide, sans texte avant ou apr√®s.

FORMAT DE R√âPONSE ATTENDU (JSON uniquement) :
[
  {"date": "15/01/2024", "libelle": "VIREMENT SALAIRE ENTREPRISE XYZ", "debit": null, "credit": 2500.00},
  {"date": "16/01/2024", "libelle": "CB CARREFOUR", "debit": 85.32, "credit": null}
]

Analyse maintenant le relev√© suivant :
"""


def extract_text_from_pdf(pdf_file) -> str:
    """
    Extrait le texte de toutes les pages d'un fichier PDF.
    
    Args:
        pdf_file: Fichier PDF upload√© via Streamlit
        
    Returns:
        str: Texte brut concat√©n√© de toutes les pages
    """
    try:
        # Lire le contenu du fichier upload√©
        pdf_bytes = pdf_file.read()
        pdf_file.seek(0)  # Reset pour permettre une relecture si n√©cessaire
        
        # Ouvrir le PDF avec PyMuPDF
        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        
        full_text = []
        for page_num in range(len(doc)):
            page = doc[page_num]
            text = page.get_text("text")
            full_text.append(f"--- Page {page_num + 1} ---\n{text}")
        
        doc.close()
        return "\n\n".join(full_text)
    
    except Exception as e:
        raise Exception(f"Erreur lors de la lecture du PDF: {str(e)}")


def is_json_truncated(json_string: str) -> bool:
    """
    V√©rifie si une cha√Æne JSON semble tronqu√©e.
    
    Args:
        json_string: Cha√Æne JSON √† v√©rifier
        
    Returns:
        bool: True si le JSON semble tronqu√©
    """
    cleaned = json_string.strip()
    
    # Enlever les marqueurs markdown pour l'analyse
    if cleaned.startswith("```json"):
        cleaned = cleaned[7:]
    if cleaned.startswith("```"):
        cleaned = cleaned[3:]
    if cleaned.endswith("```"):
        cleaned = cleaned[:-3]
    cleaned = cleaned.strip()
    
    # V√©rifier si les brackets sont √©quilibr√©s
    open_braces = cleaned.count('{') - cleaned.count('}')
    open_brackets = cleaned.count('[') - cleaned.count(']')
    
    # Si d√©s√©quilibr√©, c'est tronqu√©
    if open_braces != 0 or open_brackets != 0:
        return True
    
    # V√©rifier si √ßa se termine correctement pour un tableau JSON
    if not cleaned.endswith(']'):
        return True
    
    return False


def analyze_with_gemini(text: str, api_key: str, max_retries: int = 3) -> str:
    """
    Envoie le texte au mod√®le Gemini pour analyse avec retry automatique.
    Inclut une d√©tection de troncature avec retry.
    
    Args:
        text: Texte extrait du PDF
        api_key: Cl√© API Gemini
        max_retries: Nombre maximum de tentatives en cas d'erreur
        
    Returns:
        str: R√©ponse du mod√®le (JSON attendu)
    """
    best_response = None
    best_length = 0
    
    for attempt in range(max_retries):
        try:
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel(GEMINI_MODEL)
            
            # Construire le prompt complet
            full_prompt = SYSTEM_PROMPT + text
            
            # G√©n√©rer la r√©ponse avec mode JSON structur√©
            response = model.generate_content(
                full_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0,  # Temp√©rature 0 pour r√©sultats d√©terministes
                    max_output_tokens=32768,  # Limite max de Gemini
                    response_mime_type="application/json",  # Force Gemini √† produire un JSON valide
                )
            )
            
            response_text = response.text
            
            # Garder la meilleure r√©ponse (la plus longue)
            if len(response_text) > best_length:
                best_length = len(response_text)
                best_response = response_text
            
            # V√©rifier si la r√©ponse semble tronqu√©e
            if is_json_truncated(response_text):
                print(f"[WARNING] R√©ponse potentiellement tronqu√©e (tentative {attempt + 1}/{max_retries}), retry...")
                if attempt < max_retries - 1:
                    time.sleep(1)  # Petit d√©lai avant retry
                    continue
            
            # R√©ponse compl√®te, on peut la retourner
            return response_text
        
        except Exception as e:
            error_msg = str(e).lower()
            # V√©rifier si c'est une erreur de rate limit
            is_rate_limit = any(keyword in error_msg for keyword in ['rate limit', 'quota', '429', 'too many requests'])
            
            if is_rate_limit and attempt < max_retries - 1:
                # Attendre progressivement plus longtemps √† chaque retry
                wait_time = (attempt + 1) * 2
                time.sleep(wait_time)
                continue
            elif attempt < max_retries - 1:
                # Pour les autres erreurs, attendre un peu avant de r√©essayer
                time.sleep(1)
                continue
            else:
                # Si on a une r√©ponse partielle, la retourner plut√¥t que de lever une erreur
                if best_response:
                    print(f"[WARNING] Utilisation de la meilleure r√©ponse partielle apr√®s {max_retries} tentatives")
                    return best_response
                raise Exception(f"Erreur API Gemini apr√®s {max_retries} tentatives: {str(e)}")
    
    # Retourner la meilleure r√©ponse obtenue
    if best_response:
        return best_response
    raise Exception("Aucune r√©ponse valide obtenue de Gemini")


def repair_json(json_string: str) -> str:
    """
    R√©pare un JSON potentiellement malform√© ou tronqu√©.
    
    Args:
        json_string: Cha√Æne JSON potentiellement malform√©e
        
    Returns:
        str: Cha√Æne JSON nettoy√©e et r√©par√©e
    """
    cleaned = json_string.strip()
    
    # Enlever les marqueurs markdown
    if cleaned.startswith("```json"):
        cleaned = cleaned[7:]
    if cleaned.startswith("```"):
        cleaned = cleaned[3:]
    if cleaned.endswith("```"):
        cleaned = cleaned[:-3]
    cleaned = cleaned.strip()
    
    # Supprimer les virgules tra√Ænantes: ,} ou ,]
    cleaned = re.sub(r',\s*}', '}', cleaned)
    cleaned = re.sub(r',\s*]', ']', cleaned)
    
    # V√©rifier si le JSON est tronqu√© (brackets non ferm√©s)
    open_braces = cleaned.count('{') - cleaned.count('}')
    open_brackets = cleaned.count('[') - cleaned.count(']')
    
    if open_braces > 0 or open_brackets > 0:
        # Tronquer au dernier objet complet et fermer le tableau
        last_complete = cleaned.rfind('},')
        if last_complete > 0:
            cleaned = cleaned[:last_complete + 1] + ']'
        else:
            # Essayer de trouver le dernier objet complet sans virgule
            last_obj = cleaned.rfind('}')
            if last_obj > 0:
                cleaned = cleaned[:last_obj + 1] + ']'
    
    return cleaned


def validate_and_fix_debit_credit(df: pd.DataFrame) -> pd.DataFrame:
    """
    Valide et corrige les erreurs de classification d√©bit/cr√©dit.
    
    Priorit√© des r√®gles (du plus sp√©cifique au plus g√©n√©rique) :
    1. VIREMENTS (priorit√© haute - d√©terminent clairement la direction)
       - "VIREMENT A" ou "VIREMENT INSTANTANE A" = D√âBIT (sortie d'argent)
       - "VIREMENT DE" ou "VIREMENT INSTANTANE DE" = CR√âDIT (entr√©e d'argent)
    2. Autres patterns (priorit√© basse - seulement si pas de virement)
       - "ACHAT CB", "PRELEVEMENT", "RETRAIT" = D√âBIT
       - "CREDIT CARTE", "REMBOURSEMENT" = CR√âDIT
    
    Args:
        df: DataFrame avec les transactions
        
    Returns:
        pd.DataFrame: DataFrame corrig√©
    """
    if df.empty or "Libell√©" not in df.columns:
        return df
    
    # PRIORIT√â 1 : Patterns de virement (les plus sp√©cifiques)
    virement_debit_patterns = [
        r'VIREMENT\s+(INSTANTANE\s+)?A\s+',  # VIREMENT A ou VIREMENT INSTANTANE A
        r'VIREMENT\s+POUR\s+',  # VIREMENT POUR
    ]
    
    virement_credit_patterns = [
        r'VIREMENT\s+(INSTANTANE\s+)?DE\s+',  # VIREMENT DE ou VIREMENT INSTANTANE DE
    ]
    
    # PRIORIT√â 2 : Autres patterns (moins sp√©cifiques)
    other_debit_patterns = [
        r'ACHAT\s+CB',
        r'PRELEVEMENT',
        r'RETRAIT\s+DAB',
        # Retir√©s : COMMISSION, COTISATION, FRAIS (ambigus - la direction d√©pend du contexte)
    ]
    
    other_credit_patterns = [
        r'CREDIT\s+CARTE',
        r'REMBOURSEMENT',
    ]
    
    corrections_count = 0
    
    for idx, row in df.iterrows():
        libelle = str(row.get("Libell√©", "")).upper()
        debit = row.get("D√©bit")
        credit = row.get("Cr√©dit")
        
        pattern_matched = False
        
        # √âTAPE 1 : V√©rifier les patterns de virement EN PREMIER (priorit√© haute)
        # Virement sortant (A) = D√âBIT
        for pattern in virement_debit_patterns:
            if re.search(pattern, libelle, re.IGNORECASE):
                if pd.notna(credit) and pd.isna(debit):
                    df.at[idx, "D√©bit"] = credit
                    df.at[idx, "Cr√©dit"] = None
                    corrections_count += 1
                pattern_matched = True
                break
        
        # Virement entrant (DE) = CR√âDIT
        if not pattern_matched:
            for pattern in virement_credit_patterns:
                if re.search(pattern, libelle, re.IGNORECASE):
                    if pd.notna(debit) and pd.isna(credit):
                        df.at[idx, "Cr√©dit"] = debit
                        df.at[idx, "D√©bit"] = None
                        corrections_count += 1
                    pattern_matched = True
                    break
        
        # √âTAPE 2 : Seulement si pas de virement, v√©rifier les autres patterns
        if not pattern_matched:
            # Autres d√©bits
            for pattern in other_debit_patterns:
                if re.search(pattern, libelle, re.IGNORECASE):
                    if pd.notna(credit) and pd.isna(debit):
                        df.at[idx, "D√©bit"] = credit
                        df.at[idx, "Cr√©dit"] = None
                        corrections_count += 1
                    pattern_matched = True
                    break
            
            # Autres cr√©dits
            if not pattern_matched:
                for pattern in other_credit_patterns:
                    if re.search(pattern, libelle, re.IGNORECASE):
                        if pd.notna(debit) and pd.isna(credit):
                            df.at[idx, "Cr√©dit"] = debit
                            df.at[idx, "D√©bit"] = None
                            corrections_count += 1
                        break
    
    if corrections_count > 0:
        print(f"[INFO] {corrections_count} transaction(s) corrig√©e(s) (d√©bit/cr√©dit)")
    
    return df


def parse_llm_response(response: str, filename: str) -> pd.DataFrame:
    """
    Parse la r√©ponse JSON du LLM et la convertit en DataFrame.
    
    Args:
        response: R√©ponse texte du LLM
        filename: Nom du fichier source pour la colonne Source
        
    Returns:
        pd.DataFrame: DataFrame avec les transactions
    """
    try:
        # Utiliser repair_json pour nettoyer et r√©parer la r√©ponse
        cleaned = repair_json(response)
        
        # Parser le JSON
        transactions = json.loads(cleaned)
        
        if not isinstance(transactions, list):
            raise ValueError("La r√©ponse n'est pas une liste de transactions")
        
        if len(transactions) == 0:
            return pd.DataFrame(columns=["Date", "Libell√©", "D√©bit", "Cr√©dit", "Source"])
        
        # Cr√©er le DataFrame
        df = pd.DataFrame(transactions)
        
        # Renommer les colonnes pour le fran√ßais
        column_mapping = {
            "date": "Date",
            "libelle": "Libell√©",
            "debit": "D√©bit",
            "credit": "Cr√©dit"
        }
        df = df.rename(columns=column_mapping)
        
        # Ajouter la colonne source
        df["Source"] = filename
        
        # S'assurer que les colonnes num√©riques sont bien des nombres
        for col in ["D√©bit", "Cr√©dit"]:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # Valider et corriger les erreurs d√©bit/cr√©dit
        df = validate_and_fix_debit_credit(df)
        
        # R√©ordonner les colonnes
        expected_cols = ["Date", "Libell√©", "D√©bit", "Cr√©dit", "Source"]
        for col in expected_cols:
            if col not in df.columns:
                df[col] = None
        df = df[expected_cols]
        
        return df
    
    except json.JSONDecodeError as e:
        # Tentative de r√©cup√©ration: extraire les transactions valides avant l'erreur
        try:
            # Chercher le dernier objet JSON complet
            last_valid = cleaned.rfind('},')
            if last_valid > 0:
                truncated = cleaned[:last_valid + 1] + ']'
                transactions = json.loads(truncated)
                if isinstance(transactions, list) and len(transactions) > 0:
                    df = pd.DataFrame(transactions)
                    column_mapping = {"date": "Date", "libelle": "Libell√©", "debit": "D√©bit", "credit": "Cr√©dit"}
                    df = df.rename(columns=column_mapping)
                    df["Source"] = filename
                    for col in ["D√©bit", "Cr√©dit"]:
                        if col in df.columns:
                            df[col] = pd.to_numeric(df[col], errors='coerce')
                    expected_cols = ["Date", "Libell√©", "D√©bit", "Cr√©dit", "Source"]
                    for col in expected_cols:
                        if col not in df.columns:
                            df[col] = None
                    # Retourner les donn√©es partielles si possible
                    return df[expected_cols]
        except Exception:
            pass  # Si la r√©cup√©ration √©choue, on l√®ve l'erreur originale
        
        raise ValueError(f"Erreur de parsing JSON: {str(e)}\nR√©ponse re√ßue (500 premiers caract√®res): {response[:500]}...")
    except Exception as e:
        raise ValueError(f"Erreur lors du traitement de la r√©ponse: {str(e)}")


def aggregate_results(dataframes: list) -> pd.DataFrame:
    """
    Agr√®ge tous les DataFrames en un seul et les trie par date chronologique.
    
    Args:
        dataframes: Liste de DataFrames √† combiner
        
    Returns:
        pd.DataFrame: DataFrame unifi√© tri√© par date
    """
    if not dataframes:
        return pd.DataFrame(columns=["Date", "Libell√©", "D√©bit", "Cr√©dit", "Source"])
    
    combined = pd.concat(dataframes, ignore_index=True)
    
    # Trier par date chronologique
    if "Date" in combined.columns and len(combined) > 0:
        # Cr√©er une colonne temporaire avec les dates converties en datetime
        def parse_date(date_str):
            """Convertit une date au format JJ/MM/AAAA en datetime"""
            if pd.isna(date_str) or date_str is None:
                return pd.NaT
            try:
                return pd.to_datetime(date_str, format="%d/%m/%Y", errors='coerce')
            except:
                return pd.NaT
        
        combined['_date_sort'] = combined['Date'].apply(parse_date)
        # Trier par date (les NaT seront en dernier)
        combined = combined.sort_values('_date_sort', na_position='last')
        # Supprimer la colonne temporaire
        combined = combined.drop(columns=['_date_sort'])
        # R√©initialiser l'index
        combined = combined.reset_index(drop=True)
    
    return combined


def convert_df_to_excel(df: pd.DataFrame) -> bytes:
    """
    Convertit un DataFrame en fichier Excel.
    
    Args:
        df: DataFrame √† convertir
        
    Returns:
        bytes: Contenu du fichier Excel
    """
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='Transactions')
        
        # Ajuster la largeur des colonnes
        worksheet = writer.sheets['Transactions']
        for idx, col in enumerate(df.columns):
            max_length = max(
                df[col].astype(str).map(len).max() if len(df) > 0 else 0,
                len(col)
            ) + 2
            # Limiter la largeur maximale
            max_length = min(max_length, 50)
            worksheet.column_dimensions[chr(65 + idx)].width = max_length
    
    return output.getvalue()


def process_single_pdf(pdf_file, api_key: str, progress_lock: Lock, progress_dict: dict):
    """
    Traite un seul fichier PDF et retourne le r√©sultat.
    
    Args:
        pdf_file: Fichier PDF upload√© via Streamlit
        api_key: Cl√© API Gemini
        progress_lock: Lock pour synchroniser les mises √† jour de progression
        progress_dict: Dictionnaire partag√© pour suivre la progression
        
    Returns:
        tuple: (filename, df, error) o√π df est un DataFrame ou None, et error est un message d'erreur ou None
    """
    filename = pdf_file.name
    try:
        # √âtape 1: Extraction du texte
        text = extract_text_from_pdf(pdf_file)
        
        if not text.strip():
            return (filename, None, "Le PDF ne contient pas de texte extractible")
        
        # √âtape 2: Analyse avec Gemini
        response = analyze_with_gemini(text, api_key)
        
        # √âtape 3: Parsing de la r√©ponse
        df = parse_llm_response(response, filename)
        
        # Mettre √† jour la progression
        with progress_lock:
            progress_dict['completed'] = progress_dict.get('completed', 0) + 1
        
        return (filename, df, None)
    
    except Exception as e:
        # Mettre √† jour la progression m√™me en cas d'erreur
        with progress_lock:
            progress_dict['completed'] = progress_dict.get('completed', 0) + 1
            progress_dict['errors'] = progress_dict.get('errors', [])
            progress_dict['errors'].append((filename, str(e)))
        
        return (filename, None, str(e))


def main():
    """Fonction principale de l'application."""
    
    # CSS pour cacher la sidebar par d√©faut
    st.markdown("""
    <style>
        [data-testid="stSidebar"] {
            display: none;
        }
        [data-testid="stSidebar"][aria-expanded="true"] {
            display: block;
        }
    </style>
    """, unsafe_allow_html=True)
    
    # Titre principal
    st.title("üìä Extracteur Comptable IA")
    st.markdown("*Extrayez automatiquement les lignes comptables de vos relev√©s bancaires PDF*")
    
    # Sidebar - Configuration
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")
        
        # Gestion de la cl√© API
        api_key = os.getenv("GEMINI_API_KEY", "")
        
        if not api_key or api_key == "your_api_key_here":
            api_key = st.text_input("Cl√© API Gemini", type="password")
        
        st.divider()
        st.markdown("### ‚ö° Performance")
        num_workers = st.slider(
            "Nombre de fichiers trait√©s en parall√®le",
            min_value=1,
            max_value=8,
            value=8,
            help="Augmentez ce nombre pour traiter plus de fichiers simultan√©ment. Attention aux limites de l'API Gemini.",
            key="num_workers"
        )
        
        st.divider()
        st.markdown("### üìã Instructions")
        st.markdown("""
        1. Uploadez vos relev√©s PDF (max 15)
        2. Cliquez sur "Lancer l'analyse"
        3. T√©l√©chargez le fichier Excel
        """)
        
        st.divider()
        st.markdown("### ‚ÑπÔ∏è √Ä propos")
        st.markdown("""
        Cette application utilise **Gemini 2.5 Flash** 
        pour analyser vos relev√©s bancaires et extraire 
        automatiquement les transactions.
        
        """)
    
    # Zone principale
    st.header("üìÅ Upload des fichiers")
    
    uploaded_files = st.file_uploader(
        "Glissez vos fichiers PDF ici",
        type=["pdf"],
        accept_multiple_files=True,
        help=f"Maximum {MAX_FILES} fichiers"
    )
    
    # V√©rification du nombre de fichiers
    if uploaded_files and len(uploaded_files) > MAX_FILES:
        st.error(f"‚ùå Trop de fichiers ! Maximum autoris√© : {MAX_FILES}")
        uploaded_files = uploaded_files[:MAX_FILES]
        st.warning(f"Seuls les {MAX_FILES} premiers fichiers seront trait√©s.")
    
    if uploaded_files:
        st.info(f"üìé {len(uploaded_files)} fichier(s) s√©lectionn√©(s)")
        
        # Afficher la liste des fichiers
        with st.expander("Voir les fichiers"):
            for f in uploaded_files:
                st.text(f"‚Ä¢ {f.name}")
    
    # Bouton d'analyse
    st.divider()
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        analyze_button = st.button(
            "üöÄ Lancer l'analyse",
            type="primary",
            use_container_width=True,
            disabled=not uploaded_files or not api_key or api_key == "your_api_key_here"
        )
    
    if not api_key or api_key == "your_api_key_here":
        st.warning("‚ö†Ô∏è Veuillez configurer votre cl√© API Gemini dans la sidebar.")
    
    # Traitement
    if analyze_button and uploaded_files and api_key:
        all_dataframes = []
        errors = []
        
        # R√©cup√©rer le nombre de workers depuis la session state ou utiliser la valeur par d√©faut
        num_workers = st.session_state.get('num_workers', 8)
        
        # Barre de progression
        progress_bar = st.progress(0)
        status_text = st.empty()
        status_container = st.container()
        
        # Dictionnaire partag√© pour suivre la progression
        progress_dict = {'completed': 0, 'total': len(uploaded_files), 'errors': []}
        progress_lock = Lock()
        
        # Afficher le nombre de workers utilis√©s
        status_text.text(f"üöÄ D√©marrage du traitement parall√®le ({num_workers} fichiers simultan√©s)...")
        
        # Traitement parall√®le avec ThreadPoolExecutor
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            # Soumettre toutes les t√¢ches
            future_to_file = {
                executor.submit(process_single_pdf, pdf_file, api_key, progress_lock, progress_dict): pdf_file
                for pdf_file in uploaded_files
            }
            
            # Cr√©er un conteneur pour les messages de statut par fichier
            status_placeholders = {}
            for pdf_file in uploaded_files:
                status_placeholders[pdf_file.name] = status_container.empty()
            
            # Traiter les r√©sultats au fur et √† mesure qu'ils arrivent
            for future in as_completed(future_to_file):
                filename, df, error = future.result()
                
                # Mettre √† jour la barre de progression
                completed = progress_dict['completed']
                total = progress_dict['total']
                progress = completed / total
                progress_bar.progress(progress)
                
                # Afficher le statut
                status_text.text(f"üìä Progression : {completed}/{total} fichiers trait√©s ({int(progress * 100)}%)")
                
                if error:
                    error_msg = f"‚ùå {filename} : {error}"
                    errors.append(error_msg)
                    status_placeholders[filename].error(error_msg)
                elif df is not None and len(df) > 0:
                    all_dataframes.append(df)
                    status_placeholders[filename].success(f"‚úÖ {filename} : {len(df)} transactions extraites")
                else:
                    status_placeholders[filename].warning(f"‚ö†Ô∏è {filename} : Aucune transaction trouv√©e")
        
        progress_bar.progress(1.0)
        status_text.text("‚úÖ Traitement termin√© !")
        
        # Agr√©gation et affichage des r√©sultats
        if all_dataframes:
            st.divider()
            st.header("üìä R√©sultats")
            
            final_df = aggregate_results(all_dataframes)
            
            # Statistiques
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Total transactions", len(final_df))
            with col2:
                total_debit = final_df["D√©bit"].sum()
                st.metric("Total d√©bits", f"{total_debit:,.2f} ‚Ç¨" if pd.notna(total_debit) else "0,00 ‚Ç¨")
            with col3:
                total_credit = final_df["Cr√©dit"].sum()
                st.metric("Total cr√©dits", f"{total_credit:,.2f} ‚Ç¨" if pd.notna(total_credit) else "0,00 ‚Ç¨")
            with col4:
                st.metric("Fichiers trait√©s", len(all_dataframes))
            
            # Affichage du tableau
            st.dataframe(
                final_df,
                use_container_width=True,
                hide_index=True,
                column_config={
                    "Date": st.column_config.TextColumn("Date", width="small"),
                    "Libell√©": st.column_config.TextColumn("Libell√©", width="large"),
                    "D√©bit": st.column_config.NumberColumn("D√©bit", format="%.2f ‚Ç¨"),
                    "Cr√©dit": st.column_config.NumberColumn("Cr√©dit", format="%.2f ‚Ç¨"),
                    "Source": st.column_config.TextColumn("Source", width="medium"),
                }
            )
            
            # Bouton de t√©l√©chargement
            st.divider()
            excel_data = convert_df_to_excel(final_df)
            
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                st.download_button(
                    label="üì• T√©l√©charger le fichier Excel",
                    data=excel_data,
                    file_name="extraction_comptable.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    type="primary",
                    use_container_width=True
                )
        
        elif errors:
            st.error("‚ùå Aucune transaction n'a pu √™tre extraite. V√©rifiez vos fichiers.")


if __name__ == "__main__":
    main()
